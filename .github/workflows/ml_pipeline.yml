name: ML Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        # Install CPU-only PyTorch
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
        # Install other dependencies
        pip install numpy matplotlib pytest pytest-md
        # Install visualization tools
        sudo apt-get update
        sudo apt-get install -y graphviz
        pip install graphviz torchviz
    
    - name: Create directories
      run: |
        mkdir -p models
        mkdir -p visualizations
        mkdir -p visualizations/augmentations
    
    - name: Generate augmentation visualizations
      env:
        SKIP_VIZ: 0
        PYTHONPATH: ${GITHUB_WORKSPACE}
      run: |
        echo "Generating augmentation visualizations..."
        
        # Create Python script for augmentations
        cat > generate_augmentations.py << 'EOF'
        import os
        import sys
        import torch
        import matplotlib
        matplotlib.use('Agg')
        import base64
        from torchvision import datasets, transforms
        from utils.augmentation_viz import visualize_augmentations

        def main():
            try:
                os.makedirs('visualizations/augmentations', exist_ok=True)
                
                # Get sample image
                transform = transforms.ToTensor()
                dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
                sample_image = dataset[0][0]
                
                # Generate visualizations
                print("Generating augmentations...")
                visualize_augmentations(sample_image, num_samples=5, digit=0)
                
                # Create markdown with embedded images
                with open('visualizations/augmentations/summary.md', 'w') as f:
                    f.write("# Data Augmentation Examples\n\n")
                    
                    # List all generated images
                    for img_file in sorted(os.listdir('visualizations/augmentations')):
                        if img_file.endswith('.png'):
                            # Read image and convert to base64
                            with open(f'visualizations/augmentations/{img_file}', 'rb') as img:
                                img_data = base64.b64encode(img.read()).decode()
                            
                            # Write to markdown with embedded image
                            f.write(f"## {img_file.replace('.png', '').replace('_', ' ').title()}\n")
                            f.write(f"<img src='data:image/png;base64,{img_data}' width='600'>\n\n")
                
                return 0
                
            except Exception as e:
                print(f"Error occurred: {str(e)}", file=sys.stderr)
                import traceback
                traceback.print_exc()
                return 1

        if __name__ == "__main__":
            sys.exit(main())
        EOF
        
        # Run the script
        python generate_augmentations.py
    
    - name: Train model
      env:
        CUDA_VISIBLE_DEVICES: ""
        TORCH_DEVICE: "cpu"
        SKIP_VIZ: 1  # Skip visualization during training
      run: |
        echo "Starting model training..."
        python train.py
        echo "Training completed."
        echo "TRAINED_MODEL_PATH=$(ls -t models/*.pth | head -1)" >> $GITHUB_ENV
    
    - name: Run tests
      env:
        CUDA_VISIBLE_DEVICES: ""
        TORCH_DEVICE: "cpu"
        SKIP_VIZ: 1
      run: |
        echo "Running model tests..."
        echo "Using model: $TRAINED_MODEL_PATH"
        pytest tests/test_model.py -v -s
        echo "Testing completed."
    
    - name: Generate test summary
      run: |
        echo "## ML Pipeline Results" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "visualizations/augmentations/summary.md" ]; then
          echo "### Data Augmentation" >> $GITHUB_STEP_SUMMARY
          cat visualizations/augmentations/summary.md >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "visualizations/model_architecture.txt" ]; then
          echo "### Model Architecture" >> $GITHUB_STEP_SUMMARY
          cat visualizations/model_architecture.txt >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "test-summary.md" ]; then
          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          cat test-summary.md >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: ml-pipeline-artifacts
        path: |
          models/
          visualizations/
          visualizations/augmentations/
          test-summary.md
          .pytest_cache/
    
    - name: Verify requirements
      run: |
        echo "Checking model requirements..."
        
        # Check parameter count
        if [ -f "visualizations/model_architecture.txt" ]; then
          PARAM_COUNT=$(grep "Total Parameters:" visualizations/model_architecture.txt | awk '{print $3}' | tr -d ',')
          if [ "$PARAM_COUNT" -gt 25000 ]; then
            echo "❌ Parameter count ($PARAM_COUNT) exceeds limit of 25,000"
            exit 1
          else
            echo "✅ Parameter count ($PARAM_COUNT) within limit"
          fi
        fi
        
        # Check accuracy
        if [ -f "test-summary.md" ]; then
          if grep -q "FAILED" test-summary.md; then
            echo "❌ Model tests failed!"
            exit 1
          else
            echo "✅ All model tests passed"
          fi
        else
          echo "❌ Test summary not found"
          exit 1
        fi

    - name: Cleanup
      if: always()
      run: |
        rm -rf .pytest_cache
        rm -rf __pycache__